{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZHjqxUa18M"
      },
      "source": [
        "# Autoencoders for image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Identifying the best model on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DXV33Ra18R"
      },
      "source": [
        "### 1.1 Initialisation and visualising images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1.1 Initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from pl_bolts.models.autoencoders import AE\n",
        "\n",
        "from datasets import TransformedDataset\n",
        "\n",
        "import torch.utils.data as data\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "# SSIM\n",
        "from piqa import SSIM\n",
        "\n",
        "from datasets import CatsAndDogs\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# set the device to train on GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "input_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast = 0.3, saturation= 0.3),\n",
        "    transforms.GaussianBlur(kernel_size=9, sigma=(0.8,2)),\n",
        "    #transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast = 0.3, saturation= 0.3),\n",
        "    transforms.GaussianBlur(kernel_size=9, sigma=(0.8,2)),\n",
        "    #transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "data_dir = 'dataset'\n",
        "\n",
        "# get data\n",
        "train_dataset = datasets.CIFAR10(data_dir, train=True, download=True)\n",
        "test_dataset = datasets.CIFAR10(data_dir, train=False, download=True)\n",
        "\n",
        "# split training set into training and validation\n",
        "train_data, val_data = data.random_split(train_dataset, [45000, 5000])\n",
        "\n",
        "# using custom dataclass to transform and return both clean image and transformed image\n",
        "train_data = TransformedDataset(dataset = train_data, transform = train_transform, input_transform = input_transform)\n",
        "val_data = TransformedDataset(dataset = val_data, transform = valid_transform, input_transform = input_transform)\n",
        "test_data = TransformedDataset(dataset = test_dataset, transform = valid_transform, input_transform = input_transform)\n",
        "\n",
        "# set batch size and define the data loaders\n",
        "batch_size = 256\n",
        "test_size = 3\n",
        "\n",
        "# define data loaders\n",
        "train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=2)\n",
        "valid_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "test_loader = data.DataLoader(test_data, batch_size=test_size, shuffle=False, drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1.2 Visualising dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "p8wK_MmMa18V",
        "outputId": "6b267c66-3d3d-4a81-c504-e0756d6df631"
      },
      "outputs": [],
      "source": [
        "x_tran, y , x_in = next(iter(train_loader))\n",
        "print(\"Batch mean\", x_tran.mean(dim=[0,2,3]))\n",
        "print(\"Batch std\", x_tran.std(dim=[0,2,3]))\n",
        "print(f'Min: {x_tran.min()}, Max: {x_tran.max()}')\n",
        "\n",
        "# Convert tensor image to numpy array\n",
        "img_np = np.array(x_tran)\n",
        "  \n",
        "# plot the pixel values\n",
        "plt.hist(img_np.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"pixel values\")\n",
        "plt.ylabel(\"relative frequency\")\n",
        "plt.title(\"distribution of pixels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "tPBgh_b6a18W",
        "outputId": "0708d94e-64c7-47b0-e301-94432b018b4b"
      },
      "outputs": [],
      "source": [
        "# function to show images\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "x_tran, y, x_in = dataiter.next()\n",
        "\n",
        "# show images\n",
        "n_images = 4\n",
        "imshow(torchvision.utils.make_grid(x_in[:n_images], nrow= 4, normalize=True))\n",
        "imshow(torchvision.utils.make_grid(x_tran[:n_images], nrow= 4, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DPegYBCa18W"
      },
      "source": [
        "### 1.2 Training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgWOk9qza18Y",
        "outputId": "91a65eed-c9fd-4c9c-915d-0cbe02149e55"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optimizer\n",
        "from models import UNET, small_ae, large_ae, cnn, UNET_noskip, resnet18_ae\n",
        "\n",
        "# loss function\n",
        "mse_fn = nn.MSELoss()\n",
        "#ssim_fn = SSIM().to(device)\n",
        "\n",
        "# hyper parameters\n",
        "lr= 1e-3\n",
        "w_decay = 1e-6\n",
        "latent_dim = 512 # latent dimension\n",
        "\n",
        "# model\n",
        "#model = large_ae(3, 128, latent_dim)\n",
        "#model = small_ae(3, 128, latent_dim, out_act_fun = nn.Sigmoid)\n",
        "#model = AE(input_height=128, latent_dim = latent_dim, enc_type=)\n",
        "#model = model.from_pretrained('cifar10-resnet18')\n",
        "model = cnn()\n",
        "#model = resnet18_ae(128, latent_dim=latent_dim)\n",
        "\n",
        "optim = optimizer.Adam(model.parameters(), lr=lr, weight_decay=w_decay)\n",
        "scheduler = optimizer.lr_scheduler.ExponentialLR(optim, gamma = 0.90, last_epoch=- 1, verbose=True)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop was used to train all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training import train, test\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 30\n",
        "loss_dict={'train_loss':[],'val_loss':[]}\n",
        "\n",
        "# set up wandb\n",
        "config = dict (\n",
        "    learning_rate = lr,\n",
        "    weight_decay = w_decay,\n",
        "    epochs = num_epochs,\n",
        "    batch_size = batch_size,\n",
        "    encoded_dim = latent_dim,\n",
        "    model = model,\n",
        "    dataset = 'CIFAR10'\n",
        ")\n",
        "\n",
        "wandb.init(\n",
        "    project=\"autoencoder\", \n",
        "    entity=\"mads_birch\", \n",
        "    config = config)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss=train(\n",
        "        model = model,\n",
        "        device=device, \n",
        "        dataloader=train_loader, \n",
        "        loss_1 = mse_fn, \n",
        "        optimizer=optim)\n",
        "    \n",
        "    val_loss = test(\n",
        "        model = model, \n",
        "        device=device, \n",
        "        dataloader=valid_loader, \n",
        "        loss_1 = mse_fn,\n",
        "        plot= True)\n",
        "\n",
        "    wandb.log({\n",
        "        'training loss': train_loss,\n",
        "        'test loss': val_loss})\n",
        "\n",
        "    loss_dict['train_loss'].append(train_loss)\n",
        "    loss_dict['val_loss'].append(val_loss)\n",
        "\n",
        "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "print(\"Finsished training\")\n",
        "\n",
        "PATH = \"plot_runs/cnn_mse.pth\"\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Get mean MSE and standard deviation over 5 independent training runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop below was used to obtain mean and standard deviation over 5 independent training runs for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training import train, test\n",
        "from tqdm import tqdm\n",
        "from piqa import SSIM\n",
        "from models import UNET, small_ae, large_ae, cnn, UNET_noskip\n",
        "\n",
        "# loss function\n",
        "mse_fn = nn.MSELoss()\n",
        "#loss_fn = nn.BCELoss()\n",
        "ssim_fn = SSIM().to(device)\n",
        "\n",
        "# hyper parameters\n",
        "lr= 1e-3\n",
        "w_decay = 1e-5\n",
        "latent_dim = 512\n",
        "\n",
        "#model = large_ae(3, 128, latent_dim, out_act_fun = nn.Tanh)\n",
        "model = small_ae(3, 128, latent_dim, out_act_fun = nn.Sigmoid)\n",
        "#model = UNET()\n",
        "#model = large_cnn_ae(out_act_fun = nn.Tanh)\n",
        "#model = AE(input_height=32, latent_dim = 512).from_pretrained('cifar10-resnet18').to(device)\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "TRAIN_LOSS, VAL_LOSS = np.zeros((num_epochs,5)), np.zeros((num_epochs,5))\n",
        "\n",
        "seeds = [1,2,3,4,5]\n",
        "\n",
        "for s in seeds:\n",
        "# set up wandb\n",
        "    # set seeds\n",
        "    torch.manual_seed(s)\n",
        "    \n",
        "    import torch.optim as optimizer\n",
        "    \n",
        "    model = UNET_noskip(features=[16, 32, 64, 128]).to(device)\n",
        "\n",
        "    optim = optimizer.Adam(model.parameters(), lr=lr, weight_decay=w_decay)\n",
        "    scheduler = optimizer.lr_scheduler.ExponentialLR(optim, gamma = 0.90, last_epoch=- 1, verbose=False)\n",
        "    \n",
        "    train_loss_run, val_loss_run= [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss=train(\n",
        "            model = model,\n",
        "            device=device, \n",
        "            dataloader=train_loader, \n",
        "            loss = mse_fn,\n",
        "            optimizer=optim)\n",
        "        \n",
        "        val_loss = test(\n",
        "            model = model, \n",
        "            device=device, \n",
        "            dataloader=valid_loader, \n",
        "            loss = mse_fn,\n",
        "            plot= False)\n",
        "        \n",
        "        train_loss_run.append(train_loss)\n",
        "        val_loss_run.append(val_loss)\n",
        "\n",
        "        print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "    TRAIN_LOSS[:,s-1] = train_loss_run\n",
        "    VAL_LOSS[:,s-1] = val_loss_run\n",
        "\n",
        "    # save model to file\n",
        "    ### change this when runningnew model ###\n",
        "    PATH = \"model_runs/unet_noskip_mse_\" + str(s) + \".pth\"\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    print(f'Finsished training run #:{s}')\n",
        "\n",
        "# save error lists to file\n",
        "### change this when running new model ###\n",
        "np.savetxt('cnn_train_val_ssim.txt', np.column_stack((TRAIN_LOSS, VAL_LOSS)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Testing models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All models can now be evaluated on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from models import UNET, small_ae, large_ae, cnn\n",
        "from pl_bolts.models.autoencoders import AE\n",
        "from piqa import SSIM\n",
        "\n",
        "MSE = torch.nn.MSELoss(reduction='mean')\n",
        "ssim = SSIM()\n",
        "\n",
        "model_dict={\n",
        "    'CNN' : {'MSE': [], 'SD': [], 'n_params': []},\n",
        "    'UNET': {'MSE': [], 'SD': [], 'n_params': []},\n",
        "    'CNN/FNN Small': {'MSE': [], 'SD': [], 'n_params': []},\n",
        "    'CNN/FNN Large': {'MSE': [], 'SD': [], 'n_params': []},\n",
        "    'Resnet18': {'MSE': [], 'SD': [], 'n_params': []}\n",
        "}\n",
        "\n",
        "model_paths = ['small_ae2_mse_', './large_ae2_mse_', './cnn2_MSE_', './small_unet2_mse_', './resnet2_mse_']\n",
        "model_list = [small_ae(3 ,128, latent_dim=512), large_ae(3, 128, latent_dim = 512), cnn(), UNET(features=[16, 32, 64, 128]), AE(input_height=32)]\n",
        "model_names = ['CNN/FNN Small', 'CNN/FNN Large', 'CNN', 'UNET', 'Resnet18']\n",
        "\n",
        "for (model, name, path) in zip(model_list, model_names, model_paths):\n",
        "    print(name)\n",
        "    MSE_LIST= []\n",
        "    \n",
        "    for k in range(1,6):\n",
        "        MSEs = []\n",
        "        # load model\n",
        "        PATH = 'model_runs/'+path+str(k)+'.pth'\n",
        "        model = model\n",
        "        model.load_state_dict(torch.load(PATH))\n",
        "        for j, (noisy, _, input) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "            # evaluate the model on the test set\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                recon = model(noisy)\n",
        "            MSEs.append(MSE(recon, input).numpy())\n",
        "        MSE_LIST.append(np.mean(MSEs))\n",
        "    \n",
        "    model_dict[name]['n_params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    model_dict[name]['MSE'] = np.mean(MSE_LIST)\n",
        "    print(MSE_LIST, np.std(MSE_LIST))\n",
        "    model_dict[name]['SD'] = np.std(MSE_LIST)\n",
        "\n",
        "print(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "\n",
        "# SAVE\n",
        "with open(\"model_dict_final.pkl\", \"wb\") as pkl_handle:\n",
        "\tpickle.dump(model_dict, pkl_handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle \n",
        "# LOAD\n",
        "with open(\"model_dict_final.pkl\", \"rb\") as pkl_handle:\n",
        "\tmodel_dict = pickle.load(pkl_handle)\n",
        "    \n",
        "print(model_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot three reconstructions for each model to identify the best performing model on the CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plot import plot_results\n",
        "from models import UNET, small_ae, large_ae, cnn\n",
        "from pl_bolts.models.autoencoders import AE\n",
        "\n",
        "model_paths = ['model_runs/small_ae2_mse_1', 'model_runs/large_ae2_mse_1', 'model_runs/cnn2_MSE_1', 'model_runs/small_unet2_MSE_1', 'model_runs/resnet2_mse_1']\n",
        "model_list = [small_ae(3 ,128, latent_dim=512), large_ae(3, 128, latent_dim = 512), cnn(), UNET(features=[16, 32, 64, 128]), AE(input_height=32)]\n",
        "model_names = ['CNN/FNN Small', 'CNN/FNN Large', 'CNN', 'UNET', 'Resnet18']\n",
        "\n",
        "plot_results(data_loader = test_loader, model_list = model_list, model_paths = model_paths, model_names= model_names, normalize= True, pixel_range=(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 Effect of skip connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Initialisation and visualising images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1.1 Initialisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Cats and Dogs subset of Imagenet dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from pl_bolts.models.autoencoders import AE\n",
        "\n",
        "from datasets import TransformedDataset\n",
        "\n",
        "import torch.utils.data as data\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "# SSIM\n",
        "from piqa import SSIM\n",
        "\n",
        "from datasets import CatsAndDogs\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# set the device to train on GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "dataset = ImageFolder('c:\\\\Users\\\\sdam9\\\\Documents\\\\dl_pro\\\\dogs-vs-cats\\\\train\\\\')\n",
        "\n",
        "train_data, test_data, train_label, test_label = train_test_split(dataset.imgs, dataset.targets, test_size=0.1, random_state=9)\n",
        "train_data, val_data = data.random_split(train_data, [18000, 4500])\n",
        "\n",
        "### TRANSFORMS ###\n",
        "# resize and add noise\n",
        "input_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast = 0.3, saturation= 0.3),\n",
        "    transforms.GaussianBlur(kernel_size=9, sigma=(0.8,2))\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast = 0.3, saturation= 0.3),\n",
        "    transforms.GaussianBlur(kernel_size=9, sigma=(0.8,2))\n",
        "])\n",
        "\n",
        "train_data = CatsAndDogs(dataset = train_data, transform = train_transform, input_transform = input_transform)\n",
        "val_data = CatsAndDogs(dataset = val_data, transform = valid_transform, input_transform = input_transform)\n",
        "test_data = CatsAndDogs(dataset = test_data, transform = valid_transform, input_transform = input_transform)\n",
        "\n",
        "# set batch size and define the data loaders\n",
        "batch_size = 100\n",
        "test_size = 1\n",
        "\n",
        "# define data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=2)\n",
        "valid_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=test_size, shuffle=False, drop_last=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1.2 Visualising dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to show images\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "x_tran, y, x_in = dataiter.next()\n",
        "\n",
        "# show images\n",
        "n_images = 4\n",
        "imshow(torchvision.utils.make_grid(x_in[:n_images], nrow= 4, normalize=True))\n",
        "imshow(torchvision.utils.make_grid(x_tran[:n_images], nrow= 4, normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optimizer\n",
        "from models import UNET, small_ae, large_ae, cnn, UNET_noskip, resnet18_ae\n",
        "\n",
        "# hyper parameters\n",
        "lr= 1e-4\n",
        "w_decay = 1e-6\n",
        "latent_dim = 512 # latent dimension\n",
        "\n",
        "# model\n",
        "#model = UNET(features=[16, 32, 64, 128])\n",
        "model = UNET_noskip(features=[16, 32, 64, 128])\n",
        "\n",
        "optim = optimizer.Adam(model.parameters(), lr=lr, weight_decay=w_decay)\n",
        "scheduler = optimizer.lr_scheduler.ExponentialLR(optim, gamma = 0.90, last_epoch=- 1, verbose=True)\n",
        "\n",
        "#ssim_fn = SSIM().to(device)\n",
        "mse_fn = nn.MSELoss()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All models were trained using the below training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training import train, test\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "loss_dict={'train_loss':[],'val_loss':[]}\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss=train(\n",
        "        model = model,\n",
        "        device=device, \n",
        "        dataloader=train_loader, \n",
        "        loss_1 = mse_fn, \n",
        "        optimizer=optim)\n",
        "    \n",
        "    val_loss = test(\n",
        "        model = model, \n",
        "        device=device, \n",
        "        dataloader=valid_loader, \n",
        "        loss_1 = mse_fn,\n",
        "        plot= True)\n",
        "\n",
        "    #wandb.log({\n",
        "     #   'training loss': train_loss,\n",
        "      #  'test loss': val_loss})\n",
        "\n",
        "    loss_dict['train_loss'].append(train_loss)\n",
        "    loss_dict['val_loss'].append(val_loss)\n",
        "\n",
        "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "print(\"Finsished training\")\n",
        "\n",
        "PATH = \"final_runs/unet_noskip_mse.pth\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# SAVE\n",
        "with open(\"unet_noskip_mse.pkl\", \"wb\") as pkl_handle:\n",
        "\tpickle.dump(loss_dict, pkl_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Testing models: obtain test error and image reconstructions on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All model can now be evaluated on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from models import UNET, UNET_noskip\n",
        "from piqa import SSIM\n",
        "\n",
        "MSE = torch.nn.MSELoss(reduction='mean')\n",
        "ssim = SSIM()\n",
        "\n",
        "model_paths = ['./unet_mse', './unet_noskip_mse', './unet_ssim', './unet_noskip_ssim']\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_MSE' , 'UNET_MSE_noskip', 'UNET_SSIM', 'UNET_SSIM_noskip']\n",
        "\n",
        "\n",
        "for (model, name, path) in zip(model_list, model_names, model_paths):\n",
        "    print(name)\n",
        "\n",
        "    # load model\n",
        "    PATH = 'final_runs/'+path+'.pth'\n",
        "    model = model\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "    MSEs = []\n",
        "    SSIMs = []\n",
        "    for j, (noisy, _, input) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "        # evaluate the model on the test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            recon = model(noisy)\n",
        "        MSEs.append(MSE(recon, input).numpy())\n",
        "        SSIMs.append(ssim(recon, input).numpy())\n",
        "\n",
        "    print(f'MSE: {np.mean(MSEs)}')\n",
        "    print(f'MSSIM: {np.mean(SSIMs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image quality for each model can now be investigated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from models import UNET, UNET_noskip\n",
        "from piqa import SSIM\n",
        "from plot import plot_recon\n",
        "\n",
        "MSE = torch.nn.MSELoss(reduction='mean')\n",
        "ssim = SSIM()\n",
        "\n",
        "model_paths = ['./unet_mse', './unet_noskip_mse', './unet_ssim', './unet_noskip_ssim']\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_MSE' , 'UNET_MSE_noskip', 'UNET_SSIM', 'UNET_SSIM_noskip']\n",
        "\n",
        "\n",
        "for (model, name, path) in zip(model_list, model_names, model_paths):\n",
        "    PATH = 'final_runs/'+path+'.pth'\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "    plot_recon(model, test_loader, title = name, normalize=False, pixel_range=(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The effect of the skip-connection architecture on image reconstruction quality can now be plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plot import plot_results\n",
        "from models import UNET, UNET_noskip\n",
        "\n",
        "model_paths = ['final_runs/unet_mse', 'final_runs/unet_noskip_mse']\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_MSE' , 'UNET_MSE_noskip']\n",
        "plot_results(data_loader = test_loader, model_list = model_list, model_paths = model_paths, model_names= model_names, normalize= False, pixel_range=(0,1))\n",
        "\n",
        "\n",
        "model_paths = ['final_runs/unet_ssim', 'final_runs/unet_noskip_ssim']\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET_noskip(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_SSIM', 'UNET_SSIM_noskip']\n",
        "plot_results(data_loader = test_loader, model_list = model_list, model_paths = model_paths, model_names= model_names, normalize= False, pixel_range=(0,1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learnig is much slower when the skip-connections are removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LOAD\n",
        "with open(\"unet_mse.pkl\", \"rb\") as pkl_handle:\n",
        "\tunet_mse = pickle.load(pkl_handle)\n",
        "\n",
        "with open(\"unet_noskip_mse.pkl\", \"rb\") as pkl_handle:\n",
        "\tunet_noskip_mse = pickle.load(pkl_handle)\n",
        "\n",
        "\n",
        "x = np.arange(1,31, 1)\n",
        "plt.plot(x, unet_mse['train_loss'], label = 'U-Net')\n",
        "plt.plot(x, unet_noskip_mse['train_loss'], label = 'U-Net (no skip-connections)')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Choice of loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To investigate the effect the chosen loss function has on image quality of reconstructed images the U-Net model is trained on all different losses: MSE, SSIM, L1 and BCE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from training import train, test\n",
        "import pickle\n",
        "import torch.optim as optimizer\n",
        "from models import UNET\n",
        "\n",
        "# hyper parameters\n",
        "lr= 1e-4\n",
        "w_decay = 1e-6\n",
        "latent_dim = 512 # latent dimension\n",
        "\n",
        "# model\n",
        "model = UNET(features=[16, 32, 64, 128])\n",
        "\n",
        "optim = optimizer.Adam(model.parameters(), lr=lr, weight_decay=w_decay)\n",
        "scheduler = optimizer.lr_scheduler.ExponentialLR(optim, gamma = 0.90, last_epoch=- 1, verbose=True)\n",
        "\n",
        "ssim_loss = SSIM().to(device)\n",
        "#mse_loss = nn.MSELoss()\n",
        "L1_loss = nn.L1Loss()\n",
        "#bce_loss = nn.BCELoss()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params)\n",
        "\n",
        "num_epochs = 30\n",
        "loss_dict={'train_loss':[],'val_loss':[]}\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_loss=train(\n",
        "        model = model,\n",
        "        device=device, \n",
        "        dataloader=train_loader, \n",
        "        loss_1 = L1_loss,\n",
        "        loss_2 = ssim_loss, \n",
        "        optimizer=optim)\n",
        "    \n",
        "    val_loss = test(\n",
        "        model = model, \n",
        "        device=device, \n",
        "        dataloader = valid_loader,\n",
        "        loss_1 = L1_loss,\n",
        "        loss_2 = ssim_loss,\n",
        "        plot= True)\n",
        "\n",
        "    loss_dict['train_loss'].append(train_loss)\n",
        "    loss_dict['val_loss'].append(val_loss)\n",
        "\n",
        "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "print(\"Finsished training\")\n",
        "\n",
        "PATH = \"final_runs/unet_L1_SSIM.pth\"\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# SAVE\n",
        "with open(\"unet_L1_SSIM.pkl\", \"wb\") as pkl_handle:\n",
        "\tpickle.dump(loss_dict, pkl_handle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting image reconstructions can now be visualised next to the clean and noise images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import UNET\n",
        "from plot import plot_results\n",
        "model_paths = ['final_runs/unet_ssim', 'final_runs/unet_mse', 'final_runs/unet_L1', 'final_runs/unet_BCE',]\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_SSIM', 'UNET_MSE', 'UNET_L1Loss', 'UNET_BCE']\n",
        "plot_results(data_loader = test_loader, model_list = model_list, model_paths = model_paths, model_names= model_names, normalize= False, pixel_range=(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two best loss functions from above are combined into one loss function (SSIM + L1) and compared to the best loss (SSIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from models import UNET\n",
        "from plot import plot_results\n",
        "model_paths = ['final_runs/unet_ssim', 'final_runs/unet_L1']\n",
        "model_list = [UNET(features=[16, 32, 64, 128]), UNET(features=[16, 32, 64, 128])]\n",
        "model_names = ['UNET_SSIM',  'UNET_L1_SSIM']\n",
        "plot_results(data_loader = test_loader, model_list = model_list, model_paths = model_paths, model_names= model_names, normalize= False, pixel_range=(0,1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Notebook.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "69edf4feec6b30b9624cd80cb3056daba32bdb049729149673f3148f13f33cc3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab375dbba944de93b9e25da17395a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258a04e5d0b646d9b6a2b62632e601cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60b4d2de3ff4e4382917e902e506d06",
            "placeholder": "​",
            "style": "IPY_MODEL_3f59ed167b0f47f9885727748792b0ee",
            "value": " 170499072/? [00:11&lt;00:00, 16380912.34it/s]"
          }
        },
        "342a1f17dacd4d3cb0f4ba9ae93033ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f59ed167b0f47f9885727748792b0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4588af52e12d40a89a02c1f0c739beb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e57dd42db148ebaf237902613e8c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87181df4fd7e41f682e25cef4aefb7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f1e4c5fac74618922aae4cd22b0ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87181df4fd7e41f682e25cef4aefb7ab",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53e57dd42db148ebaf237902613e8c87",
            "value": 170498071
          }
        },
        "9d8b237a029e4c6cbf253384c0a35159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4588af52e12d40a89a02c1f0c739beb7",
            "placeholder": "​",
            "style": "IPY_MODEL_342a1f17dacd4d3cb0f4ba9ae93033ed",
            "value": ""
          }
        },
        "9dd6ca286fe643edbcc12abdc97d0d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d8b237a029e4c6cbf253384c0a35159",
              "IPY_MODEL_93f1e4c5fac74618922aae4cd22b0ba3",
              "IPY_MODEL_258a04e5d0b646d9b6a2b62632e601cb"
            ],
            "layout": "IPY_MODEL_07ab375dbba944de93b9e25da17395a9"
          }
        },
        "d60b4d2de3ff4e4382917e902e506d06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
